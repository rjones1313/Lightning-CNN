{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc5053c8-100f-4a30-b29d-688820b6532b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T18:49:19.911842Z",
     "iopub.status.busy": "2025-09-29T18:49:19.911676Z",
     "iopub.status.idle": "2025-09-29T18:50:33.008458Z",
     "shell.execute_reply": "2025-09-29T18:50:33.007523Z",
     "shell.execute_reply.started": "2025-09-29T18:49:19.911823Z"
    }
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import scipy.stats as stats\n",
    "import netCDF4\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import sys\n",
    "import random\n",
    "import os\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib.path import Path\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from eofs.xarray import Eof\n",
    "import time\n",
    "import h5py\n",
    "from scipy.signal import correlation_lags\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import linregress\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c864d573-ee4a-4be6-aaf8-da9cb6d92aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/giantstep5/rjones98/machine_learning/input_datasets/ERA5')\n",
    "\n",
    "ds_daily = xr.open_dataset('ds_fulldegree_3hr_2010_2023_v3.nc')\n",
    "ds_daily = ds_daily.isel(lon=slice(0, 148), lat=slice(0, 104))\n",
    "\n",
    "dataset_cnn = ds_daily\n",
    "\n",
    "feature_name   = ['cape', 'precipitation', 'lsm', 'rh', 'shear', 't2m', 'wcd']\n",
    "output_name    = ['ltg']\n",
    "\n",
    "indices_train_val = np.arange(0, 35064)  #2010 to 2021\n",
    "idx_test = np.arange(35064, 40904)       #2022 and 2023\n",
    "idx_train, idx_val = train_test_split(indices_train_val, test_size=0.25, random_state=13) #Save 25% of training dataset for validation\n",
    "zdim, _, ydim, xdim = ds_daily[feature_name].to_array().shape\n",
    "\n",
    "train_dataset_cnn_X = dataset_cnn[feature_name].isel(time=idx_train)\n",
    "val_dataset_cnn_X = dataset_cnn[feature_name].isel(time=idx_val)\n",
    "test_dataset_cnn_X  = dataset_cnn[feature_name].isel(time=idx_test)\n",
    "train_dataset_cnn_y = dataset_cnn[output_name].isel(time=idx_train)\n",
    "val_dataset_cnn_y = dataset_cnn[output_name].isel(time=idx_val)\n",
    "test_dataset_cnn_y  = dataset_cnn[output_name].isel(time=idx_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a780ab4-b6e6-40a3-9d32-ebc0a1b596c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 13\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Define the PyTorch model\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, zdim, ydim, xdim):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.encoder0 = self.encoder_block(zdim, 32)\n",
    "        self.encoder1 = self.encoder_block(32, 16)\n",
    "        self.center = self.conv_block(16, 8)\n",
    "        self.decoder1 = self.decoder_block(8, 16)\n",
    "        self.decoder0 = self.decoder_block(16, 32)\n",
    "        self.outputs = nn.Sequential(\n",
    "            nn.Conv2d(32, 1, kernel_size=1, padding=0),\n",
    "#            nn.ReLU()  # Ensures non-negative output\n",
    "        )\n",
    "\n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def encoder_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            self.conv_block(in_channels, out_channels),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "    def decoder_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoder0 = self.encoder0(x)\n",
    "        encoder1 = self.encoder1(encoder0)\n",
    "        center = self.center(encoder1)\n",
    "        decoder1 = self.decoder1(center)\n",
    "        decoder0 = self.decoder0(decoder1)\n",
    "        outputs = self.outputs(decoder0)\n",
    "        return outputs\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "zdim, ydim, xdim = 7, 104, 148  # Adjust these based on your data\n",
    "model = SimpleCNN(zdim, ydim, xdim).to('cpu')\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2462dbed-a7ce-4319-af2f-3640f390910e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x, m, s):\n",
    "    return (x - m).groupby('time.month') / s\n",
    "\n",
    "def unnormalize(x, m, s):\n",
    "    return (x * s).groupby('time.month') + m\n",
    "\n",
    "train_dataset_cnn_X_norm = (\n",
    "    (train_dataset_cnn_X.groupby('time.month')\n",
    "     - train_dataset_cnn_X.groupby('time.month').mean(dim={'time','lat','lon'})\n",
    "     ).groupby('time.month')\n",
    "     / train_dataset_cnn_X.groupby('time.month').std(dim={'time','lat','lon'})\n",
    ").fillna(0)\n",
    "\n",
    "val_dataset_cnn_X_norm = (\n",
    "    (val_dataset_cnn_X.groupby('time.month')\n",
    "     - train_dataset_cnn_X.groupby('time.month').mean(dim={'time','lat','lon'})\n",
    "     ).groupby('time.month')\n",
    "     / train_dataset_cnn_X.groupby('time.month').std(dim={'time','lat','lon'})\n",
    ").fillna(0)\n",
    "\n",
    "test_dataset_cnn_X_norm = (\n",
    "    (test_dataset_cnn_X.groupby('time.month')\n",
    "     - train_dataset_cnn_X.groupby('time.month').mean(dim={'time','lat','lon'})\n",
    "     ).groupby('time.month')\n",
    "     / train_dataset_cnn_X.groupby('time.month').std(dim={'time','lat','lon'})\n",
    ").fillna(0)\n",
    "\n",
    "train_dataset_cnn_y_norm = (\n",
    "    (train_dataset_cnn_y.groupby('time.month')\n",
    "     - train_dataset_cnn_y.groupby('time.month').mean(dim={'time','lat','lon'})\n",
    "     ).groupby('time.month')\n",
    "     / train_dataset_cnn_y.groupby('time.month').std(dim={'time','lat','lon'})\n",
    ").fillna(0)\n",
    "\n",
    "val_dataset_cnn_y_norm = (\n",
    "    (val_dataset_cnn_y.groupby('time.month')\n",
    "     - train_dataset_cnn_y.groupby('time.month').mean(dim={'time','lat','lon'})\n",
    "     ).groupby('time.month')\n",
    "     / train_dataset_cnn_y.groupby('time.month').std(dim={'time','lat','lon'})\n",
    ").fillna(0)\n",
    "\n",
    "test_dataset_cnn_y_norm = (\n",
    "    (test_dataset_cnn_y.groupby('time.month')\n",
    "     - train_dataset_cnn_y.groupby('time.month').mean(dim={'time','lat','lon'})\n",
    "     ).groupby('time.month')\n",
    "     / train_dataset_cnn_y.groupby('time.month').std(dim={'time','lat','lon'})\n",
    ").fillna(0)\n",
    "\n",
    "# Convert to numpy arrays and move axis\n",
    "tr_X = np.moveaxis(np.array(train_dataset_cnn_X_norm.to_array()), 0, -1)\n",
    "val_X = np.moveaxis(np.array(val_dataset_cnn_X_norm.to_array()), 0, -1)\n",
    "test_X = np.moveaxis(np.array(test_dataset_cnn_X_norm.to_array()), 0, -1)\n",
    "tr_y = np.moveaxis(np.array(train_dataset_cnn_y_norm.to_array()), 0, -1)\n",
    "val_y = np.moveaxis(np.array(val_dataset_cnn_y_norm.to_array()), 0, -1)\n",
    "test_y = np.moveaxis(np.array(test_dataset_cnn_y_norm.to_array()), 0, -1)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "tr_X_tensor = torch.tensor(tr_X, dtype=torch.float32).to('cpu')\n",
    "val_X_tensor = torch.tensor(val_X, dtype=torch.float32).to('cpu')\n",
    "test_X_tensor = torch.tensor(test_X, dtype=torch.float32).to('cpu')\n",
    "tr_y_tensor = torch.tensor(tr_y, dtype=torch.float32).to('cpu')\n",
    "val_y_tensor = torch.tensor(val_y, dtype=torch.float32).to('cpu')\n",
    "test_y_tensor = torch.tensor(test_y, dtype=torch.float32).to('cpu')\n",
    "\n",
    "tr_X_tensor = tr_X_tensor.permute(0, 3, 1, 2)\n",
    "val_X_tensor = val_X_tensor.permute(0, 3, 1, 2)\n",
    "test_X_tensor = test_X_tensor.permute(0, 3, 1, 2)\n",
    "tr_y_tensor = tr_y_tensor.permute(0, 3, 1, 2)\n",
    "val_y_tensor = val_y_tensor.permute(0, 3, 1, 2)\n",
    "test_y_tensor = test_y_tensor.permute(0, 3, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f4594c-3481-4a1c-9eff-ec144b899cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, train_inputs, train_targets, val_inputs, val_targets, batch_size=128, epochs=50, patience=5):\n",
    "    model.train()\n",
    "    dataset_size = len(train_inputs)\n",
    "    val_size = len(val_inputs)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 5\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        permutation = torch.randperm(dataset_size)\n",
    "        for i in range(0, dataset_size, batch_size):\n",
    "            indices = permutation[i:i + batch_size]\n",
    "            batch_inputs, batch_targets = train_inputs[indices], train_targets[indices]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_inputs)\n",
    "            loss = criterion(outputs, batch_targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, val_size, batch_size):\n",
    "                val_batch_inputs = val_inputs[i:i + batch_size]\n",
    "                val_batch_targets = val_targets[i:i + batch_size]\n",
    "                val_outputs = model(val_batch_inputs)\n",
    "                val_loss += criterion(val_outputs, val_batch_targets).item()\n",
    "        \n",
    "        val_loss /= (val_size // batch_size)\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Training Loss: {loss.item()}, Validation Loss: {val_loss}')\n",
    "              \n",
    "train(model, criterion, optimizer, tr_X_tensor, tr_y_tensor, val_X_tensor, val_y_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73800137-767a-42c4-97e7-a489995351b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions_norm = model(test_X_tensor)\n",
    "\n",
    "# Convert predictions to numpy array\n",
    "predictions_norm_np = predictions_norm.numpy()\n",
    "predictions_norm_np = predictions_norm_np.squeeze(1)\n",
    "\n",
    "# Define dimensions and coordinates (modify based on your data)\n",
    "dims = ['time', 'lat', 'lon']\n",
    "coords = {\n",
    "    'time': test_dataset_cnn_y['time'].values,\n",
    "    'lat': test_dataset_cnn_y['lat'].values,\n",
    "    'lon': test_dataset_cnn_y['lon'].values\n",
    "}\n",
    "\n",
    "# Create an xarray DataArray\n",
    "predictions_norm_da = xr.DataArray(predictions_norm_np, dims=dims, coords=coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4e1ee2-92a2-4d40-a80c-3391f459f3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_unnorm_da = (\n",
    "    ( ( (predictions_norm_da-predictions_norm_da.mean(dim={'time','lat','lon'}))/predictions_norm_da.std(dim={'time','lat','lon'})).groupby('time.month')\n",
    "     *train_dataset_cnn_y['ltg'].groupby('time.month').std(dim={'time','lat','lon'})\n",
    "     ).groupby('time.month')\n",
    "     +train_dataset_cnn_y['ltg'].groupby('time.month').mean(dim={'time','lat','lon'})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd066c6-a4d8-4fd7-934d-86a866bf8390",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_unnorm_da = predictions_unnorm_da.where(predictions_unnorm_da >= 0, other = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b907ea57-c030-4d03-833f-f1527dc6876e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_unnorm_ds = predictions_unnorm_da.to_dataset(name='ltg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorchenv)",
   "language": "python",
   "name": "pytorchenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "375.797px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
